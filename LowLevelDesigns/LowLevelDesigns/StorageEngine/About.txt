Let's walk through a very basic key-value storage engine similar to in concept to HaloDb.

What is HaloDb?
HaloDb is a persistent key-value store inspired by Bitcask, optimized for high throughput and low latency.
Its main concept:
1. Append-only log files: Writes go to a log file(like a WAL - write - ahead log).
2. In-memory index: A hash map in memory pointing keys to their offset in the log file.
3. Compaction: Periodically reqrites active keys into new log files and avoid file bloat.

(Persistent in a database or storage engine means: The data you write is not lost when the program stops or the machine shuts down. It survives crashes, reboots or restarts).


Phase 1 Commit:
Functions:
1. Write (key,value) to log file where the key is mapped to an offset in the file where the (key,value) are stored.
2. Read data from log file
3. Rebuild index
4. Maintain a hash-map in memory with key:offset pairs

Phase 2 commit:
1. Write-Ahead Logs (WAL) -- For durability and crash recovery
2. Compaction --> reclaims disc space by keeping only latest values for each key
3. MemoryMappedFiles --> faster input/output operations
4. TTL --> Key expiration support
5. Concurrency Handling --> Thread safe read / write
6. Snapshots / Checkpoints

Step 1: Add Write-Ahead Log(WAL):
We'll write every operation PUT/DELETE to a wal.log file before we apply changes to our data.db file.
Changes needed:
--> Add a wal.log file
--> On Put(), write it to wal.log before writing it to data.db
--> On startup, replay WAL to recover state.


Step 2: Add Compaction support
Since the database is append only, multiple updates to a key, results in duplicate old keys and values in the file
This processs will remove old keys and retain only the lates values for each key. Updates the index accordingly.
What we will do ?
--> A method Compact():
 --> Iterates through current key value pairs
 --> Writes them to a new file
 --> Swaps the with the original one
 --> clears the WAL after compaction

 Step 3: Memory-Mapped files for faster reads
 What are memory mapped files ? 
 -->They map a file into memory
 -->You can read directly a file without repeatedly opening streams
 -->This is fast for frequent read heavy workloads

 Notes on Step3:
 * When you read/write files traditionally:
	var stream = new FileStream("data.db",FileMode.Open);
	stream.Read(buffer,0,buffer.Length);

	You're doing:
	--> Syscall to OS(read);
	--> Copies data from disc to kernel memory
	--> Every read/write has overhead and buffering

	That works fine but if you are reading millions of times than its slow

	Memory mapped file = File mapped to virtual memory:
	--> OS maps the file to your process's virutal memory.
	--> POints that memory to the file's content on disc
	When you read from memory-mapped region, OS loads the relevant part of the file into RAM, now you can keep reading/writing without delay

	--> modified pages are flushed back to disc (optional)




