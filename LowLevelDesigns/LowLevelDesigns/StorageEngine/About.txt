Let's walk through a very basic key-value storage engine similar to in concept to HaloDb.

What is HaloDb?
HaloDb is a persistent key-value store inspired by Bitcask, optimized for high throughput and low latency.
Its main concept:
1. Append-only log files: Writes go to a log file(like a WAL - write - ahead log).
2. In-memory index: A hash map in memory pointing keys to their offset in the log file.
3. Compaction: Periodically reqrites active keys into new log files and avoid file bloat.

(Persistent in a database or storage engine means: The data you write is not lost when the program stops or the machine shuts down. It survives crashes, reboots or restarts).


Phase 1 Commit:
Functions:
1. Write (key,value) to log file where the key is mapped to an offset in the file where the (key,value) are stored.
2. Read data from log file
3. Rebuild index
4. Maintain a hash-map in memory with key:offset pairs

Phase 2 commit:
1. Write-Ahead Logs (WAL) -- For durability and crash recovery
2. Compaction --> reclaims disc space by keeping only latest values for each key
3. MemoryMappedFiles --> faster input/output operations
4. TTL --> Key expiration support
5. Concurrency Handling --> Thread safe read / write
6. Snapshots / Checkpoints

Step 1: Add Write-Ahead Log(WAL):
We'll write every operation PUT/DELETE to a wal.log file before we apply changes to our data.db file.
Changes needed:
--> Add a wal.log file
--> On Put(), write it to wal.log before writing it to data.db
--> On startup, replay WAL to recover state.


Step 2: Add Compaction support
Since the database is append only, multiple updates to a key, results in duplicate old keys and values in the file
This processs will remove old keys and retain only the lates values for each key. Updates the index accordingly.
What we will do ?
--> A method Compact():
 --> Iterates through current key value pairs
 --> Writes them to a new file
 --> Swaps the with the original one
 --> clears the WAL after compaction

 Step 3: Memory-Mapped files for faster reads
 What are memory mapped files ? 
 -->They map a file into memory
 -->You can read directly a file without repeatedly opening streams
 -->This is fast for frequent read heavy workloads

 Notes on Step3:
 * When you read/write files traditionally:
	var stream = new FileStream("data.db",FileMode.Open);
	stream.Read(buffer,0,buffer.Length);

	You're doing:
	--> Syscall to OS(read);
	--> Copies data from disc to kernel memory
	--> Every read/write has overhead and buffering

	That works fine but if you are reading millions of times than its slow

	Memory mapped file = File mapped to virtual memory:
	--> OS maps the file to your process's virutal memory.
	--> POints that memory to the file's content on disc
	When you read from memory-mapped region, OS loads the relevant part of the file into RAM, now you can keep reading/writing without delay

	--> modified pages are flushed back to disc (optional)


Step 4: Add TTL (Time-To-Live)

What it does?
--> Lets you set a key with an expiry
--> Expired keys wont be return
--> Expired keys are skipped during compaction


Step 5: Add Snapshot / Checkpoint
What we'll do ?
--> Save current index +TTL metadata
--> On startup optionally load the snapshot instead of replying WAL

Step 6: Add Bloom Filters

-->What is a bloom filter ? --> A bloom filter is a space efficient probabilistic data structure used to test whether a key might exist in a dataset --- zero false negatives and some false positives.
-->Why its useful ? Prevents unneccessary disc checks. If bloom filter says, "definetly not present" we avoid disc checks
--> Example use: Before doing a get(key), we ask the bloom filter "Is the key possibly in db?"
If it says “not present” → the key is definitely not in the DB.
If it says “might be present” → the key may or may not be there (we confirm by checking disk).
It's super fast and uses very little memory, at the cost of false positives (but no false negatives).

Step 6: TTL Cleanup
TTL is a lifespan, that you assign to a key, after which the key is considered expired.

--> Why do we need background cleanup?
--> Keeps your data fresh
--> Frees up space by removing expired keys
--> Prevents stale values from being accessed

Strategy:
Store each key's expiration timestamp
Spawn a background thread than runs every few seconds
It scans the in-memory TTL index
Deletes the expired entries and logs it



